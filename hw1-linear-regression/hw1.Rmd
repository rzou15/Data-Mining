---
title: "CS 422 Section 01 hw1"
output: html_notebook
author: Rui Zou  A20351034
---

##2.1 Problem 1
####(a)(b)
```{r}
setwd("/Users/rzou/Academics/Courses/CS422 - Data Mining/hw1")

college = read.csv("College.csv", header=T, sep=",")
rownames(college) <- college[ , 1]
college <- college[ , -1]
college
```
####(c)
```{r}
summary(college)
```
####(c)
```{r}
par(family='Arial Unicode MS')
pairs(college[ , 1:10])
```

```{r}
# boxplot of perc.alumni by public/private
par(family='Arial Unicode MS')
boxplot(perc.alumni~Private, data=college, main="Alumni Percent by Private/Public",
        xlab="Private or not", ylab="Percent of alumni who donate")

```
We can see that alumni who go to private schools donate more to their colleges.

```{r}
# boxplot of PhD by public/private
par(family='Arial Unicode MS')
boxplot(PhD~Private, data=college, main="Employing PhD by Private/Public",
        xlab="Private or not", ylab="Percent of faculty with PhDs")
```
We can see that public colleges employ more PhDs.

```{r}
Elite <- rep("No", nrow(college))
Elite[college$Top10perc > 50] <- "Yes"
Elite <- as.factor(Elite)
college <- data.frame(college, Elite)
summary(college)
```
There are 78 elite universities.

```{r}
par(family='Arial Unicode MS')
par(mfrow=c(2,2))
hist(college$Enroll, xlab="Number of new students enrolled", main="Histogram of Enroll")
hist(college$F.Undergrad, xlab="Number of full-time undergraduates", main="Histogram of Full-time Undergraduates")
hist(college$Expend, xlab="Instructional expenditure per student", main="Histogram of Expend")
hist(college$Grad.Rate, xlab="Graduation rate", main="Histogram of Graduation Rate")
```

```{r}
par(family='Arial Unicode MS')
boxplot(Grad.Rate~Elite, data=college, main="Graduation Rate by Elite",
        xlab="Elite or not", ylab="Graduation Rate")
```
We can see that Elite colleges have remarkably better graduation rates than non-elite colleges.

##2.2 Problem 2
####(a)
```{r}
df = read.csv("nba.csv", header=T, sep=",")
head(df)
```
```{r}
# test correlation between all the numeric predictors and the response.
for (i in 1:ncol(df)){
  if (is.numeric(df[[i]]))
    cat(colnames(df)[i],"\t", cor(x=df[[i]], y=df$PTS, method='pearson'), "\n")
}
```
We choose the predictor "FG" because it has the largest positive correlation coefficient.
```{r}
par(family='Arial Unicode MS')
attach(df)
plot(FG, PTS, main="Correlation plot (PTS by FG)", xlab="Field Goals", ylab="Points Scored", pch=19)
```

```{r}
# simple linear regression
slr <- lm(PTS~FG, data=df)
summary(slr)
```
The model fits the data very well. The estimated standard error for FG is 0.04715. The p-values for FG is extremely low <2e-16. The RSS is 2.241. 
####(b)
```{r}
par(family='Arial Unicode MS')
attach(df)
plot(FG, PTS, main="Simple Linear Regression PTS~FG", xlab="Field Goals", ylab="Points Scored", pch=19)
abline(lm(PTS~FG), col="red") # regression line (y~x)
```
```{r}
# train-test split
set.seed(1122)
index <- sample(1:nrow(df), 250)
train <- df[index, ]
test <- df[-index, ]
```
####(c)
```{r}
require(psych)

print(corr.test(df[,8:ncol(df)], use="pairwise", method="pearson", adjust="holm", alpha=.05, ci=TRUE))
```
We can see that other predictors like FGA, MIN, and FT, have high correlation with PTS. So we choose them as regressors.
```{r}
par(family='Arial Unicode MS')
par(mfrow=c(1,3))
attach(df)
plot(FGA, PTS, main="Correlation plot (PTS~FGA)", xlab="Field Goals Attempted", ylab="Points Scored", pch=19, col='red')
plot(MIN, PTS, main="Correlation plot (PTS~MIN)", xlab="Minutes Played", ylab="Points Scored", pch=19, col='green')
plot(FT, PTS, main="Correlation plot (PTS~FT)", xlab="Free Throws Made", ylab="Points Scored", pch=19, col='blue')
```
####(d)
```{r}
# multiple regression
mr <- lm(PTS ~ FG + FGA + MIN + FT, data=train)
summary(mr)
```
According to p-values in the last column, lower p-values indicates larger statistical significance, so FG, FT, and MIN are statistically significant predictors, while FGA are not, which will be eliminated.

Evaluation: The high R-squared value 0.9791 statistically measures the data are close to the fitted regression line. The overall p-value of F-statistic 2919 on 4 and 245 Degree of Freedom is extremely small, i.e. smaller that 0.001 so we can reject H0 (Additional variables except intercept do not provide value taken together) and say that overall addition of variables is significantly improving the model. So the model fits very well.

####(e)
```{r}
# modified model, eliminate FGA
mr2 <- lm(PTS ~ FG + MIN + FT, data=train)
par(family='Arial Unicode MS')
plot(resid(mr2), pch=18, col='orange', main="Scatterplot of Residuals", xlab='Object Index', ylab='Residual Values')
```
There is no obvious shape of scatterplot of the residuals. But objects tend to be densely distributed near 0 residual, and gradually get sparsed as far with 0 residual.  

####(f)
```{r}
par(family='Arial Unicode MS')
hist(resid(mr2), main="Histogram of Residuals", xlab="Residuals")
```
Yes, the histogram of residuals follows an approximate Gaussian distribution.
####(g)
```{r}
pred <- predict(mr2, test)
pred_r <- as.integer(round(pred))
eval <- data.frame(pred_r, test[, ncol(test)])
eval
```
```{r}
sum(eval[,1]==eval[,2])
```
Among 19 test objects, there are 7 objects whose fitted PTS value exactly matches its actaul PTS value.

####(h)
```{r}
# calculate residual vector for the predictions on the test set
resid_t <- eval[,2] - pred
rss = sum(resid_t^2)
tss = sum((eval[,2] - mean(eval[,2]))^2)
n = nrow(test)                  # number of objects 
k = length(mr2$coefficients)    # number of model parameters
# The degree of freedom for residual, total, and model, are n-k, n-1, k-1, respectively.
fstat = (tss-rss)/(k-1) / (rss/(n-k))
rse = sqrt(rss/(n-k))
  
cat("RSS (Residual Sum of Squares):", "\t", rss, "\n")
cat("TSS (Total Sum of Squares):", "\t", tss, "\n")
cat("F-statistic:", "\t", fstat, "\n")
cat("RSE (Residual Standard Error):", "\t", rse)

```




















